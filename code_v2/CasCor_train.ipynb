{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederick/anaconda3/lib/python3.7/site-packages/openpyxl/worksheet/_reader.py:312: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training loss: 1.93\n",
      "training accuracy: 20.00\n",
      "validation loss: 1.94\n",
      "validation accuracy: 11.59%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/frederick/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:55: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of hidden units:    1\n",
      "training loss: 1.93\n",
      "training accuracy: 20.71\n",
      "validation loss: 1.94\n",
      "validation accuracy: 10.14%\n",
      "\n",
      "number of hidden units:    2\n",
      "training loss: 1.93\n",
      "training accuracy: 20.88\n",
      "validation loss: 1.94\n",
      "validation accuracy: 11.59%\n",
      "\n",
      "number of hidden units:    3\n",
      "training loss: 1.93\n",
      "training accuracy: 20.18\n",
      "validation loss: 1.94\n",
      "validation accuracy: 13.04%\n",
      "\n",
      "number of hidden units:    4\n",
      "training loss: 1.93\n",
      "training accuracy: 20.88\n",
      "validation loss: 1.94\n",
      "validation accuracy: 10.14%\n",
      "\n",
      "number of hidden units:    5\n",
      "training loss: 1.93\n",
      "training accuracy: 21.95\n",
      "validation loss: 1.94\n",
      "validation accuracy: 13.04%\n",
      "\n",
      "number of hidden units:    6\n",
      "training loss: 1.93\n",
      "training accuracy: 20.18\n",
      "validation loss: 1.94\n",
      "validation accuracy: 8.70%\n",
      "\n",
      "number of hidden units:    7\n",
      "training loss: 1.93\n",
      "training accuracy: 20.71\n",
      "validation loss: 1.94\n",
      "validation accuracy: 11.59%\n",
      "\n",
      "number of hidden units:    8\n",
      "training loss: 1.93\n",
      "training accuracy: 21.24\n",
      "validation loss: 1.94\n",
      "validation accuracy: 14.49%\n",
      "\n",
      "number of hidden units:    9\n",
      "training loss: 1.93\n",
      "training accuracy: 15.93\n",
      "validation loss: 1.94\n",
      "validation accuracy: 13.04%\n",
      "\n",
      "number of hidden units:    10\n",
      "training loss: 1.93\n",
      "training accuracy: 18.05\n",
      "validation loss: 1.94\n",
      "validation accuracy: 18.84%\n",
      "\n",
      "number of hidden units:    11\n",
      "training loss: 1.93\n",
      "training accuracy: 21.24\n",
      "validation loss: 1.94\n",
      "validation accuracy: 15.94%\n",
      "\n",
      "number of hidden units:    12\n",
      "training loss: 1.93\n",
      "training accuracy: 15.22\n",
      "validation loss: 1.94\n",
      "validation accuracy: 13.04%\n",
      "\n",
      "number of hidden units:    13\n",
      "training loss: 1.93\n",
      "training accuracy: 15.75\n",
      "validation loss: 1.95\n",
      "validation accuracy: 11.59%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import openpyxl\n",
    "from scipy.stats import zscore\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import KFold\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# load the dataset\n",
    "# SFEW_rename_title_line.xlsx is a slightly  modified file on the original dataset which renames the first line\n",
    "data = pd.read_excel('SFEW_rename_title_line.xlsx', engine='openpyxl')\n",
    "\n",
    "# separate into PHOG and LPQ data sets\n",
    "data_Y_both = data.iloc[:, 1]\n",
    "data_LPQ = pd.concat([data_Y_both, data.iloc[:, 2:7]], axis=1)\n",
    "data_PHOG = pd.concat([data_Y_both, data.iloc[:, 7:]], axis=1)\n",
    "data_LPQ = data_LPQ.dropna()\n",
    "data_PHOG = data_PHOG.dropna()\n",
    "\n",
    "\n",
    "# combined dataset\n",
    "data = data.dropna()\n",
    "data = data.iloc[:, 1:]\n",
    "\n",
    "\n",
    "\n",
    "####################### CasCor model ###############################\n",
    "\n",
    "class TensorDataset(torch.utils.data.Dataset):\n",
    "    '''\n",
    "    Used for backprop in CasCor\n",
    "    '''\n",
    "    def __init__(self,train_features,train_labels):\n",
    "        self.X = train_features\n",
    "        self.Y = train_labels\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "\n",
    "def backprop(x_, y_, weights, loss_func=torch.nn.CrossEntropyLoss(),\n",
    "             learning_rate=0.001, epochs=500, is_cov=False, batch_size=128):\n",
    "    '''\n",
    "    Used for training input weights to a candidate unit and output weights to the output units\n",
    "    '''\n",
    "    x = torch.tensor(x_).float()\n",
    "    if is_cov:\n",
    "        y = torch.tensor(y_)\n",
    "    else:\n",
    "        y = torch.tensor(y_).long()\n",
    "        \n",
    "    train_loader = torch.utils.data.DataLoader(TensorDataset(x_, y_), batch_size, shuffle = True)\n",
    "    net = torch.nn.Linear(weights.shape[0], weights.shape[1])\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for _, (batch_x, batch_y) in enumerate(train_loader):\n",
    "            X = batch_x.float()\n",
    "            if is_cov:\n",
    "                Y = batch_y  # for training the input weights to candidate units\n",
    "            else:\n",
    "                Y = batch_y.long()\n",
    "\n",
    "            optimizer.zero_grad()        \n",
    "            y_pred = net(X)\n",
    "        \n",
    "            loss = loss_func(y_pred, Y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    final_pred = net.forward(x)\n",
    "    final_loss = loss_func(final_pred, y)\n",
    "\n",
    "    return net.weight.data.numpy(), final_pred.detach().numpy(), final_loss.item(), net\n",
    "\n",
    "\n",
    "def covariance_loss(pred, target):\n",
    "    '''Calculate the correlation S, and take its reciprocal'''\n",
    "    pred_mean = torch.mean(pred, dim=0)\n",
    "    target_mean = torch.mean(target, dim=0)\n",
    "    target_cor = target - target_mean\n",
    "\n",
    "    S = torch.sum(torch.abs(torch.sum((pred - pred_mean) * target_cor, axis=0)), axis=0)\n",
    "    return 1/S\n",
    "\n",
    "\n",
    "def train_hidden(x, y, predicted, pool_size=3):\n",
    "    '''\n",
    "    Train the input weights to a candidate unit and returns the candidate with maximum correlation S\n",
    "    '''\n",
    "    n, m = x.shape\n",
    "    candidate_pool = []\n",
    "    best_idx = 0\n",
    "    best_S = math.inf\n",
    "    net = None\n",
    "\n",
    "    # select the predicted probability for the correct label\n",
    "    predicted = F.softmax(torch.tensor(predicted).float(), dim=1).detach().numpy()\n",
    "    predicted = np.choose(y, predicted.T).reshape(-1, 1)\n",
    "\n",
    "    for i in range(pool_size):\n",
    "        weights = np.random.randn(m, 1)\n",
    "\n",
    "        # Calculate the residual error for correlation\n",
    "        err = torch.tensor(predicted - np.ones(predicted.shape))\n",
    "\n",
    "        # pass in the residual errors as a target\n",
    "        weights, predicted, inv_S, net = backprop(x, err, weights, loss_func=covariance_loss, is_cov=True)\n",
    "\n",
    "        # select the minimum 1/S == select the maximum S\n",
    "        candidate_pool.append((weights, predicted))\n",
    "        if inv_S < best_S:\n",
    "            best_S = inv_S\n",
    "            best_idx = i\n",
    "\n",
    "    return candidate_pool[best_idx], net\n",
    "\n",
    "\n",
    "class CasCorNet(object):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.weights = self.init_weights()  # weights of inputs to the outputs and hidden units to the outputs\n",
    "        self.no_hidden_units = 0\n",
    "        self.loss_func = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "    def set_data(self, X_train, y_train, X_val, y_val, X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_val = X_val\n",
    "        self.y_val = y_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "\n",
    "    def init_weights(self):\n",
    "        '''\n",
    "        Random initial the weights\n",
    "        '''\n",
    "        weights = np.random.randn(self.input_size, self.output_size)\n",
    "        return weights\n",
    "\n",
    "    def accuracy(self, y_pred, Y):\n",
    "        predicted = np.argmax(y_pred, 1)\n",
    "        total = len(y_pred)\n",
    "        correct = sum(predicted == Y)\n",
    "        return 100 * correct / total\n",
    "\n",
    "    def test_model(self, net):\n",
    "        '''Use the reserved test set to evaluate the final model'''\n",
    "        if net: # net is not None, otherwise return directly\n",
    "            y_test_val = net.forward(torch.tensor(self.X_test).float())\n",
    "            test_loss = self.loss_func(y_test_val, torch.tensor(self.y_test).long())\n",
    "            print(\"#\" * 10)\n",
    "            print(\"Final test loss: \", test_loss.item())\n",
    "            print(\"Final test accuracy:\", self.accuracy(y_test_val.detach().numpy(), self.y_test))\n",
    "            \n",
    "            return test_loss.item(), self.accuracy(y_test_val.detach().numpy(), self.y_test)\n",
    "\n",
    "    def train(self):\n",
    "        all_train_loss = []\n",
    "        all_train_acc = []\n",
    "        all_val_loss = []\n",
    "        all_val_acc = []\n",
    "\n",
    "        iteration = 0\n",
    "        acceptable_loss = 0.1\n",
    "        max_iterations = 32\n",
    "\n",
    "        while True:\n",
    "            new_weights, predict, loss, net = backprop(self.X_train, self.y_train, self.weights,\n",
    "                                                       loss_func=nn.CrossEntropyLoss())\n",
    "\n",
    "            # update output units' weights\n",
    "            self.weights = new_weights\n",
    "            \n",
    "            # calculate train accuracy and loss\n",
    "            train_acc = self.accuracy(predict, self.y_train)\n",
    "            all_train_loss.append(loss)\n",
    "            all_train_acc.append(train_acc)\n",
    "            print(\"training loss: {:.2f}\".format(loss))\n",
    "            print(\"training accuracy: {:.2f}\".format(train_acc))\n",
    "\n",
    "            # calculate validation accuracy and loss\n",
    "            y_pred_val = net.forward(torch.tensor(self.X_val).float())\n",
    "            val_loss = self.loss_func(y_pred_val, torch.tensor(self.y_val).long())\n",
    "            val_acc = self.accuracy(y_pred_val.detach().numpy(), self.y_val)\n",
    "            all_val_loss.append(val_loss)\n",
    "            all_val_acc.append(val_acc)\n",
    "            print(\"validation loss: {:.2f}\".format(val_loss.item()))\n",
    "            print(\"validation accuracy: {:.2f}%\".format(val_acc))\n",
    "            print()\n",
    "\n",
    "            # termination criterion\n",
    "            if val_loss < acceptable_loss:\n",
    "                break\n",
    "            if iteration == max_iterations:\n",
    "                break\n",
    "\n",
    "            # otherwise add a new hidden unit\n",
    "            self.X_train = self.add_hidden_unit(self.X_train, self.y_train, predict)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "        # use the test set to evaluate the model\n",
    "        return self.test_model(net)\n",
    "        \n",
    "\n",
    "    def augment_input(self, x, v):\n",
    "        '''\n",
    "        Helper method for augment v to x\n",
    "        '''\n",
    "        new_x = np.zeros((x.shape[0], x.shape[1] + 1))\n",
    "        new_x[:, :-1] = x\n",
    "        new_x[:, -1] = v\n",
    "\n",
    "        return new_x\n",
    "\n",
    "    def add_hidden_unit(self, X, Y, predict):\n",
    "        # train the input weights to the hidden unit and augment the hidden unit's value to the input\n",
    "        (weights, neuron_value), net = train_hidden(X, Y, predict)\n",
    "        # augment hidden unit value to the input <-> frozen and store input weights to the hidden unit\n",
    "        new_X = self.augment_input(X, neuron_value.reshape(-1))\n",
    "\n",
    "        # update validation set input <-> frozen and store input weight to the hidden unit\n",
    "        val_neuron_value = net.forward(torch.tensor(self.X_val).float()).detach().numpy()\n",
    "        self.X_val = self.augment_input(self.X_val, val_neuron_value.reshape(-1))\n",
    "\n",
    "        # update test set input <-> frozen and store input weight to the hidden unit\n",
    "        test_neuron_value = net.forward(torch.tensor(self.X_test).float()).detach().numpy()\n",
    "        self.X_test = self.augment_input(self.X_test, test_neuron_value.reshape(-1))\n",
    "\n",
    "        self.input_size += 1\n",
    "\n",
    "        # update weights connecting to the output units for later weight updating\n",
    "        new_weights = self.init_weights()\n",
    "        new_weights[:-1, :] = self.weights.T\n",
    "        self.weights = new_weights\n",
    "        self.no_hidden_units += 1\n",
    "        print(\"number of hidden units:   \", self.no_hidden_units)\n",
    "\n",
    "        return new_X\n",
    "\n",
    "def add_bias(X):\n",
    "    '''\n",
    "    Add bias to the inputs before training \n",
    "    '''\n",
    "    tmp = np.ones((X.shape[0], X.shape[1] + 1))\n",
    "    tmp[:, :-1] = X\n",
    "    return tmp\n",
    "\n",
    "\n",
    "\n",
    "def cascor_train(data_train_X, data_train_Y, data_val_X, data_val_Y, data_test_X, data_test_Y):\n",
    "    '''\n",
    "    Add bias to the input, then start training\n",
    "    '''\n",
    "    data_train_X = add_bias(data_train_X.to_numpy())\n",
    "    data_train_Y = data_train_Y.to_numpy()\n",
    "    data_val_X = add_bias(data_val_X.to_numpy())\n",
    "    data_val_Y = data_val_Y.to_numpy()\n",
    "    data_test_X = add_bias(data_test_X.to_numpy())\n",
    "    data_test_Y = data_test_Y.to_numpy()\n",
    "    \n",
    "    net = CasCorNet(len(data_train_X[0]), len(np.unique(data_train_Y)))\n",
    "    net.set_data(data_train_X, data_train_Y, data_val_X, data_val_Y, data_test_X, data_test_Y)\n",
    "    return net.train()\n",
    "\n",
    "def cascor_test_train_split(data):\n",
    "    # split data set into train, test\n",
    "    msk = np.random.rand(len(data)) < 0.95\n",
    "    data_train = data[msk]\n",
    "    data_test = data[~msk]\n",
    "    \n",
    "    # split the train data set into train, validation\n",
    "    msk = np.random.rand(len(data_train)) < 0.9\n",
    "    data_valid = data_train[~msk]\n",
    "    data_train = data_train[msk]\n",
    "    \n",
    "    data_train_X = data_train.iloc[:, 1:]\n",
    "    data_train_Y = data_train.iloc[:, 0] - 1\n",
    "    data_valid_X = data_valid.iloc[:, 1:]\n",
    "    data_valid_Y = data_valid.iloc[:, 0] - 1\n",
    "    data_test_X = data_test.iloc[:, 1:]\n",
    "    data_test_Y = data_test.iloc[:, 0] - 1\n",
    "    \n",
    "    return cascor_train(data_train_X, data_train_Y, data_valid_X, data_valid_Y, data_test_X, data_test_Y)\n",
    "    \n",
    "def cascor_starter(data_LPQ, data_PHOG):\n",
    "    '''Train LPQ and PHOG separately and diplay the results'''\n",
    "    LPQ_loss, LPQ_acc = cascor_test_train_split(data_LPQ)\n",
    "    PHOG_loss, acc = cascor_test_train_split(data_PHOG)\n",
    "\n",
    "    print(\"LPQ loss acc: \", LPQ_loss, LPQ_acc)\n",
    "    print(\"PHOG loss acc: \", PHOG_loss, PHOG_acc)\n",
    "\n",
    "def cascor_combined_starter(data):\n",
    "    '''Train combined dataset and display the result'''\n",
    "    loss, acc = cascor_test_train_split(data)\n",
    "    print(\"Combined loss: \", loss)\n",
    "    print(\"Combined acc: \", acc)\n",
    "\n",
    "\n",
    "\n",
    "# cascor_starter(data_LPQ, data_PHOG)\n",
    "cascor_combined_starter(data)\n",
    "\n",
    "####################### End of the CasCor model ###############################\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
